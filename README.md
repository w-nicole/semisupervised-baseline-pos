
# Added text: as of semisupervised-baseline-pos repository

For further changes, see below.

The general following edits were applied, in addition to the ones described below:

- Code was added to remove sliding window behavior, and instead sentences were considered individually.

Note that notes on license were not yet added to every file from original repository as many files will be deleted before final state. Ones that are retained are marked as project progresses.

# Added text: as of crosslingual-private repository

Commit 2a7b53bfcd1f12c34da410f6799bc18d9adae746

A copy of the crosslingual-nlp repository, with the following general edits:

- Changes to minor hyperparameters to the corresponding paper at https://arxiv.org/abs/1904.09077l.
    - But NOT changes to other processing or details described in the paper that differ from the original code.
- A different environment was used, so the .yml was deleted.
- Simplified `example/surprising-mbert/evaluate.sh` script due to no need for non-POS evaluation.
- A few scripts and files added for convenience of analyzing and executing various runs (`run_multiple_check_variation.sh`, anything with `download` in its name in `src`, `src/scratchwork`)
- Change to `constant.py` to get labels to run with older UD.
- Changes to the way that types are represented in `util.py` to resolve compile errors/possible library inconsistencies.

The above was generated by going through the complete diff as of 3/28/22 (found in the crosslingual-private repository) and changes were described in relevant files.

For the state of the repository before changes, see commit c029e10e909039946a7555ddad87d99e9e0f9fc9 in that repository.

For the state of the repository used for replication numbers before UD change, see commit b1a9af51796f62b9436f2860e44b86891b77be5b in that repository.

# Crosslingual NLP

This repo supports various cross-lingual transfer learning & multilingual NLP models. It powers the following papars.

- Mahsa Yarmohammadi*, Shijie Wu*, Marc Marone, Haoran Xu, Seth Ebner, Guanghui Qin, Yunmo Chen, Jialiang Guo, Craig Harman, Kenton Murray, Aaron Steven White, Mark Dredze, and Benjamin Van Durme. [*Everything Is All It Takes: A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction*](https://arxiv.org/abs/2109.06798). EMNLP. 2021. ([Experiments Detail](example/data-projection))
- Shijie Wu and Mark Dredze. [*Do Explicit Alignments Robustly Improve Multilingual Encoders?*](https://arxiv.org/abs/2010.02537) EMNLP. 2020. ([Experiments Detail](example/contrastive-alignment))
- Shijie Wu and Mark Dredze. [*Are All Languages Created Equal in Multilingual BERT?*](https://arxiv.org/abs/2005.09093) RepL4NLP. 2020. ([Experiments Detail](example/low-resource-in-mbert))
- Shijie Wu*, Alexis Conneau*, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. [*Emerging Cross-lingual Structure in Pretrained Language Models*](https://arxiv.org/abs/1911.01464). ACL. 2020. ([Experiments Detail](example/emerging-crossling-struct))
- Shijie Wu and Mark Dredze. [*Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT*](https://arxiv.org/abs/1904.09077). EMNLP. 2019. ([Experiments Detail](example/surprising-mbert))



## Miscellaneous

- Environment (conda): `environment.yml`
- Pre-commit check: `pre-commit run --all-files`

## License

MIT
